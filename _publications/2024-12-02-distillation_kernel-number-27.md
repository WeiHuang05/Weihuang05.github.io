---
title: "Provable and Efficient Dataset Distillation for Kernel Ridge Regression"
collection: publications
permalink: /publication/2024-12-02-distillation_kernel-number-27
excerpt: 'In this paper, by focusing on dataset distillation for kernel ridge regression (KRR), we show that one data point per class is already necessary and sufficient to recover the original models performance in many settings.'
date: 2024-12-02
venue: 'NeurIPS'
paperurl: 'https://openreview.net/pdf?id=WI2VpcBdnd'
---

[Download paper here](https://openreview.net/pdf?id=WI2VpcBdnd)
