---
title: "Provable and Efficient Dataset Distillation for Kernel Ridge Regression"
collection: publications
permalink: /publication/2024-12-02-distillation_kernel-number-27
excerpt: 'Our results show that SLTrain adds minimal extra parameters and memory costs compared to pretraining with low-rank parameterization, 
yet achieves substantially better performance, which is comparable to full-rank training.'
date: 2024-12-01
venue: 'NeurIPS'
paperurl: 'https://openreview.net/pdf?id=WI2VpcBdnd'
---

[Download paper here](https://openreview.net/pdf?id=WI2VpcBdnd)
